{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÄ: verifica que si hayas instalado las librer√≠as que vas a necesitar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitulo 2. Proyecto de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se realizar√° un modelo para predecir la media de precios en las viviendas de los diferentes municipos de Londres, Reino Unido üá¨üáß."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">1. Descargar los datos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las bases de datos para este proyecto se puede encontrar en este enlace: https://www.kaggle.com/justinas/housing-in-london\n",
    "\n",
    "Tambi√©n se pueden consultar todas las bases de datos de este curso en GitHub:https://github.com/a2Proyectos/MachineLearning_Data\n",
    "\n",
    "- housing_in_london_yearly_variables.csv, con los datos que necesitamos para hacer la regresi√≥n.\n",
    "- London_Borough_Excluding_MHW.shp, con los datos que necesitamos para graficar Londres.\n",
    "- Capitulo_2/housing_in_london_monthly_variables.csv, con los datos de la media salarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importamos nuestras librerias principales panda, numpy, matplotlib, os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definimos una funci√≥n para extraer datos.\n",
    "#DOWNLOAD_ROOT es la base del GitHub donde vamos a estar descargando las bases de datos.\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ANFESADI15/A2/main/C2/\"\n",
    "#Complementos con la direcci√≥n especifica de la base de datos que queremos.\n",
    "LONDON_SALARY = \"MachineLearning_Data-main/MachineLearning_Data-main/Capitulo_2/housing_in_london_yearly_variables.csv\"\n",
    "LONDON_HOUSING = \"MachineLearning_Data-main/MachineLearning_Data-main/Capitulo_2/housing_in_london_monthly_variables.csv\"\n",
    "LONDON_MAP = os.path.abspath(\"\") + \"\\map\\London_Borough_Excluding_MHW.shp\"\n",
    "\n",
    "\n",
    "def extraer_datos(root,database):\n",
    "    csv_path = root + database\n",
    "    return pd.read_csv(csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:green\">2. Vistazo a la Base de Datos</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Juntamos nuestra base de datos de la media salarial, con la de datos de Londres.\n",
    "df1 = extraer_datos(DOWNLOAD_ROOT, LONDON_SALARY)\n",
    "df2 = extraer_datos(DOWNLOAD_ROOT, LONDON_HOUSING)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtrar los datos \n",
    "df1 = df1.filter(items=[\"median_salary\",\"area\",\"date\"])\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fusionar los dos dataframe\n",
    "data = pd.merge(df1,df2)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener informaci√≥n de los datos.\n",
    "\"population_size\",\"life_satisfaction\",\"number_of_houses\"\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÄ:Comienza a ver el tipo de variable, de eso depende el an√°lisis que le demos o si necesitamos cambiarlas a otro tipo de variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una forma muy com√∫n para saber que tipo de datos contiene alguna variable de tipo objeto, es contar sus valores, ejemplo:\n",
    "data[\"area\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una forma muy utilizada para obtener informaci√≥n de nuestros datos num√©ricos es el m√©todo describe(), ejemplo:\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza un Histograma para visualizar los datos \n",
    "#matplotlib inline para aquellos que estan con jupyter notebook\n",
    "data.hist(bins=50,figsize=(15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">3. Crear un set de entrenamiento y de prueba </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerden que nuestro set de prueba lo pondremos de lado por ahora, sin verlo. No sean tramposos ‚ùå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos la funci√≥n para dividir los datos train_test_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos por ejemplo, el 30% de los datos para el set de prueba\n",
    "#Random_state es la semilla que se usa para generar n√∫meros aleatorios.\n",
    "set_ent, set_prueba = train_test_split(data, test_size=0.3, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirmamos la divisi√≥n\n",
    "print(len(set_ent),len(set_prueba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos nuestro set de prueba y nuestro set de entrenamiento ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">3.1 Evitar Sesgo </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para categorizar una variable, ejemplo con 5 niveles. \n",
    "data[\"salary_cat\"] = pd.cut(data[\"median_salary\"],\n",
    "                           bins=[0., 10000, 20000, 30000, 40000,\n",
    "                                np.inf],\n",
    "                           labels=[1, 2, 3, 4, 5])\n",
    "#Hacer un histograma de las categor√≠as\n",
    "data[\"salary_cat\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verificar que no existen datos en el bin #1\n",
    "data[\"salary_cat\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefinir en 4 categor√≠as, quitando en donde no hay datos (el bin#1)\n",
    "data = data.dropna(subset=['median_salary'])\n",
    "data = data.reset_index()\n",
    "data[\"salary_cat\"] = pd.cut(data[\"median_salary\"],\n",
    "                           bins=[10000., 20000., 30000., 40000.,\n",
    "                                np.inf],\n",
    "                           labels=[1, 2, 3, 4])\n",
    "data[\"salary_cat\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifica que si se redefinieron las categor√≠as \n",
    "data[\"salary_cat\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dividir datos bas√°ndonos en nuestras categor√≠as de salarios\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos nuestro objeto para que lo divida en 30% y solo haga una divisi√≥n\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos nuestras variables bas√°ndonos en nuestras categor√≠as\n",
    "for ent_index, prueba_index in split.split(data,data[\"salary_cat\"]):\n",
    "    cat_set_ent = data.loc[ent_index]\n",
    "    cat_set_prueba = data.loc[prueba_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobaci√≥n. Ya en porcentaje\n",
    "cat_set_prueba[\"salary_cat\"].value_counts()/ len(cat_set_prueba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos dataframe para trabajar con el set de entrenamiento\n",
    "df = cat_set_ent.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">4. Visualizar los Datos Gr√°ficamente </span>\n",
    "\n",
    "\n",
    "Para esto vamos a necesitar, en conjunto con nuestro dataset LONDON_MAP, una nueva libreria que se llama geopandas, el cual exteiende la libreria pandas, para trabajar con datos geoespaciales, se puede encontrar m√°s informaci√≥n en: https://geopandas.org/getting_started/introduction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Importar geopandas\n",
    "import geopandas as gpd\n",
    "\n",
    "#Leer el mapa\n",
    "londres_map = gpd.read_file(LONDON_MAP)\n",
    "londres_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos el mapa\n",
    "londres_map.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustamos los nombres de las columnas para desp√∫es hacer un merge.\n",
    "#utilizar lower para cambiar de may√∫sculas a min√∫sculas \n",
    "londres_map.columns = londres_map.columns.str.lower()\n",
    "londres_map = londres_map.rename({'name': 'area', 'gss_code': 'code'}, axis=1)\n",
    "londres_map[\"area\"] = londres_map[\"area\"].str.lower()\n",
    "\n",
    "\n",
    "#Seleccionar columnas necesarias\n",
    "londres_map = londres_map.filter(items=[\"area\",\"code\",\"hectares\",\"geometry\"])\n",
    "londres_map.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar datos de nuestro set de entrenamiento\n",
    "df_m = df.groupby('area').agg({'average_price': ['mean'], 'houses_sold': 'sum'})\n",
    "\n",
    "# Le asignamos nombre a las columnas del nuevo dataframe y reseteamos el indice\n",
    "df_m.columns = ['average_price', 'houses_sold']\n",
    "df_m.reset_index(inplace = True)\n",
    "df_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar dtaframes\n",
    "londres_map = pd.merge(londres_map,df_m,on=\"area\")\n",
    "londres_map.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gr√°fica del promedio de los precios en las casas \n",
    "#Cuando se grafica en geopandas hay muchos argumentos, no se desesperen si no los recuerdan, es normal. \n",
    "plt = londres_map.plot(column = 'average_price', cmap = 'Reds', edgecolor = 'maroon',\n",
    "               legend = True, legend_kwds = {'label': 'Precio', 'orientation' : 'horizontal'})\n",
    "plt.set_title('Media de los precios en las casas')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí∏: Recuerden que los precios son mayores en el centro de la cuidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficar ahora el total de las casa vendidas (utilizar el c√≥digo anterior para no repetir)\n",
    "plt = londres_map.plot(column = 'houses_sold', cmap = 'Blues', edgecolor = 'maroon',\n",
    "               legend = True, legend_kwds = {'label': 'Precio', 'orientation' : 'horizontal'})\n",
    "plt.set_title('Total de casas vendidas')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:** Incluir en el an√°lisis los salarios promedio por zona (apoyarse del video 21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">5. Medir la Correlaci√≥n </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear matriz de correlaci√≥n\n",
    "matriz = df.corr(method='pearson')\n",
    "\n",
    "# Comparar correlaci√≥n\n",
    "matriz[\"average_price\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar seaborn \n",
    "import seaborn as sns\n",
    "\n",
    "# Crear vector\n",
    "mask = np.triu(np.ones_like(matriz, dtype = bool))\n",
    "\n",
    "# Graficar\n",
    "plt = sns.heatmap(matriz, mask = mask, annot = True, cmap = 'YlGnBu_r')\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar pandas \n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "#Para graficar scatter_matrix...\n",
    "columns = ['average_price', 'median_salary', 'no_of_crimes', 'houses_sold']\n",
    "scatter_matrix(df[columns], figsize = (12, 12), color = '#D52B06', alpha = 0.3, \n",
    "               hist_kwds = {'color':['bisque'], 'edgecolor': 'firebrick'});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Por si quieren ver una gr√°fica en espec√≠fico m√°s a detalle\n",
    "df.plot(kind=\"scatter\",y=\"median_salary\",x=\"average_price\",alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">6. Combinaci√≥n de Variables </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Armar una columna para hacer las combinaciones que necesitamos\n",
    "df[\"vendidas_poblacion\"] = df[\"population_size\"] / df[\"houses_sold\"]\n",
    "# Crear matriz de correlaci√≥n\n",
    "matriz = df.corr(method='pearson')\n",
    "matriz[\"average_price\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Blue\">7. Transformaci√≥n de Datos </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear Dataframe de predictores y variable a predecir ‚úÇÔ∏è\n",
    "df_label = cat_set_ent['average_price']\n",
    "#Nuevo dataframe sin average_price\n",
    "df = cat_set_ent.drop('average_price', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Darte una idea de a qu√© variables le hace faltan datos\n",
    "df.info()\n",
    "#Calcular la suma de todos los vac√≠os  \n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quitar el n√∫mero de cr√≠menes por su alta cantidad de datos vac√≠os \n",
    "df=df.drop(\"no_of_crimes\", axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tomamos la media \n",
    "median = df[\"houses_sold\"].median()\n",
    "#Llenamos los valores con la media\n",
    "df['houses_sold'].fillna(median,inplace=True)  #opci√≥n 3\n",
    "#Verifica que no hay datos vac√≠os\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerden que volvimos a ejecutar la l√≠nea de c√≥digo de \"df = cat_set_ent.drop('average_price', axis=1)\" para hacer este ejercicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BIENVENIDOS A SCIKIT <3 Ser√°n unos expertos al final. \n",
    "# Rellenar valores con scikit\n",
    "#1. Recuerden: importar lo que vayas a utiliza, en este caso SimpleImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 2. Crear objeto, en este caso se llamar√° imputer\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "df.isna().sum()\n",
    "df= df.drop([\"no_of_crimes\"],axis=1)\n",
    "df.info()\n",
    "\n",
    "# data num√©rico\n",
    "df_num = df.drop([\"area\",\"date\",\"code\"],axis=1)\n",
    "\n",
    "#Ejecuta Imputer\n",
    "imputer.fit(df_num)\n",
    "\n",
    "#Aplicar transform para rellenar las medianas \n",
    "X = imputer.transform(df_num)\n",
    "\n",
    "#Regresarlo a dataframe \n",
    "df_tr = pd.DataFrame(X, columns=df_num.columns, index=df_num.index)\n",
    "df_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Blue\">8. Manejo de texto y valores categ√≥ricos </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definir que variable vamos a cambiar a valor num√©rico\n",
    "df_cat=df[[\"area\"]]\n",
    "#Convertir variables de texto en num√©ricas\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "df_oe = ordinal_encoder.fit_transform(df_cat)\n",
    "#verificar que si se haya transformado\n",
    "df_oe[0:10]\n",
    "\n",
    "#Por si quieres ver como funciona el encoder\n",
    "ordinal.encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONE HOT ENCODER, es importante recordar y entender esta funci√≥n porque la usaremos en todo el curso\n",
    "#Convertir variables categ√≥ricas en binarias\n",
    "#Importar OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#Crear objeto\n",
    "encoder = OneHotEncoder()\n",
    "#Ajustar\n",
    "df_1hot = encoder.fit_transform(df_cat)\n",
    "#obligar a que nos muestre la matriz \n",
    "df_1hot.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Blue\">9. Escalaci√≥n de variables </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Blue\">9.1 Normalizaci√≥n </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#Creamos el objeto \n",
    "scaler = MinMaxScaler()\n",
    "#Hacer un dataframe con la transformaci√≥n\n",
    "pd.DataFrame(scaler.fit_transform(prueba),columns=prueba.columns,index=prueba.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Blue\">9.2 Estandarizaci√≥n </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Estandarizaci√≥n (Es el m√°s ultizado)\n",
    "#Importar StandardScaler (presten especial atenci√≥n a StandardScaler lo estaremos viendo muy seguido) \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#Crea objeto\n",
    "scaler = StandardScaler()\n",
    "#Hacer un dataframe con la transformaci√≥n\n",
    "pd.DataFrame(scaler.fit_transform(prueba),columns=prueba.columns,index=prueba.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Blue\">10. Pipeline </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear funci√≥n de pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([(\"remover\",RemoverOutliers()),\n",
    "                      (\"rellenar\",SimpleImputer(strategy=\"median\")),\n",
    "                       (\"escalar\",StandardScaler())])\n",
    "pd.DataFrame(pipeline.fit_transform(prueba),columns=prueba.columns,index=prueba.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Transformer (nos permite transformar varias columnas al mismo tiempo y luego juntarlas)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "num = list(df_num)\n",
    "cat = [\"area\"]\n",
    "\n",
    "pipeline_completo = ColumnTransformer([\n",
    "    (\"num\", pipeline, num),\n",
    "    (\"cat\", OneHotEncoder(), cat)\n",
    "])\n",
    "#Hacer un datframe que usaremos para la regresi√≥n linea\n",
    "df_preparado = pipeline_completo.fit_transform(df)\n",
    "#Visualiza los datos \n",
    "df_preparado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:purple\">11. Seleccionar y entrenar modelos </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hacer Regresi√≥n Lineal (AL FIN)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg_lin = LinearRegression()\n",
    "reg_lin.fit(df_preparado, df_label)\n",
    "\n",
    "algunos_datos = df.iloc[:5]\n",
    "datos_predecir = df_label.iloc[:5]\n",
    "datos_transformados = pipeline_completo.transform(algunos_datos)\n",
    "print(\"Predicci√≥n:\",reg_lin.predict(datos_transformados))\n",
    "print(\"\\nOriginales:\",list(datos_predecir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An√°lisis preambultario de los errores\n",
    "algunos_datos=df.iloc[:5]\n",
    "dato_predecir=df_label.iloc[:5]\n",
    "datos_transformados=pipeline_completo.transform(algunos_datos)\n",
    "\n",
    "vp=list(reg_lin.predict(datos_transformados))\n",
    "vr=list(dato_predecir)\n",
    "\n",
    "vp=pd.Series(vp)\n",
    "vr=pd.Series(vr)\n",
    "\n",
    "\n",
    "abs((vr-vp)/vr).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:purple\">11.1 RMSE </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el promedio de la suma de los errores al cuadrado RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "prediccion = reg_lin.predict(df_preparado)\n",
    "error = mean_squared_error(df_label,prediccion)\n",
    "error = np.sqrt(error)\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sacar el promedio \n",
    "df_label.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcular el porcentaje de acierto\n",
    "error/df.label.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:purple\">11.2 √Årbol de Decisi√≥n </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "#crear objeto \n",
    "reg_arbol = DecisionTreeRegressor()\n",
    "#correrlo\n",
    "reg_arbol.fit(df_preparado,df_label)\n",
    "prediccion = reg_arbol.predict(df_preparado)\n",
    "#calcular el error\n",
    "error = mean_squared_error(df_label,prediccion)\n",
    "error = np.sqrt(error)\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:purple\">11.3 Validaci√≥n Cruzada </span>\n",
    "\n",
    "¬°Anota esto porque es importante!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar cross_val_score, creo objeto, corro mi funci√≥n\n",
    "from sklearn.model_selection import cross_val_score\n",
    "resultados = cross_val_score(reg_arbol, df_preparado, df_label, scoring=\"neg_mean_squared_error\",cv = 10)\n",
    "rmse = np.sqrt(-resultados)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcular el porcentaje de acierto de validaci√≥n cruzada \n",
    "rmse.mean()/df.label.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:purple\">11.4 Bosque Aleatorio </span>\n",
    "¬°Este tambi√©n es importante!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar RandomForestRegressor, creo objeto, corro mi funci√≥n\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "reg_forest = RandomForestRegressor()\n",
    "reg_forest.fit(df_preparado,df_label)\n",
    "prediccion = reg_forest.predict(df_preparado)\n",
    "\n",
    "#calcular el error\n",
    "error = mean_squared_error(df_label,prediccion)\n",
    "error = np.sqrt(error)\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcular el el promedio de la suma de los errores al cuadrado\n",
    "resultados = cross_val_score(reg_forest, df_preparado, df_label, scoring=\"neg_mean_squared_error\",cv = 10)\n",
    "rmse = np.sqrt(-resultados)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcular el porcentaje de acierto de bosque aleatorio\n",
    "rmse.mean()/df.label.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:purple\">12. Afinar el modelo </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:purple\">12.1 Grid Search</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [{\n",
    "    'n_estimators': [3,10,30], 'max_features': [2,4,6,8]\n",
    "}]\n",
    "grid_search = GridSearchCV(reg_forest,param_grid,cv=5,scoring='neg_mean_squared_error',return_train_score=True)\n",
    "grid_search.fit(df_preparado,df_label)\n",
    "#Calcular el mejor par√°metro\n",
    "grid_search.best_params_\n",
    "#Ver el error\n",
    "np.sqrt(-grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:purple\">12.2 set de prueba</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬°AHORA SI! A utilizar el set de prueba. Es la √∫ltima parte \n",
    "\n",
    "#Define cu√°l es el modelo final \n",
    "modelo_final = grid_search.best_estimator_\n",
    "\n",
    "#Aqu√≠ definimos cu√°l ser√° nuestra variable a predecir y las predichas \n",
    "Y = cat_set_prueba[\"average_price\"].copy()\n",
    "X = cat_set_prueba.drop(\"average_price\",axis=1)\n",
    "\n",
    "#Ahora, s√≠. El pipeline para limpieza de datos \n",
    "X_preparada = pipeline_completo.transform(X)\n",
    "prediccion_final = modelo_final.predict(X_preparada)\n",
    "\n",
    "#Por √∫ltimo, ver el error del modelo \n",
    "mse_final = mean_squared_error(Y, prediccion_final)\n",
    "rmse = np.sqrt(-mse_final)\n",
    "np.sqrt(mse_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ver el porcentaje de acierto\n",
    "rmse/Y.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
